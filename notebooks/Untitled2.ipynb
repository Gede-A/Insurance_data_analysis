{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9184722b-183a-4d24-b9ae-042874612cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UnderwrittenCoverID  PolicyID     TransactionMonth  IsVATRegistered  \\\n",
      "0               145249     12827  2015-03-01 00:00:00             True   \n",
      "\n",
      "  Citizenship          LegalType Title Language                 Bank  \\\n",
      "0              Close Corporation    Mr  English  First National Bank   \n",
      "\n",
      "       AccountType  ...         ExcessSelected CoverCategory   CoverType  \\\n",
      "0  Current account  ...  Mobility - Windscreen    Windscreen  Windscreen   \n",
      "\n",
      "             CoverGroup              Section                          Product  \\\n",
      "0  Comprehensive - Taxi  Motor Comprehensive  Mobility Metered Taxis: Monthly   \n",
      "\n",
      "  StatutoryClass StatutoryRiskType  TotalPremium TotalClaims  \n",
      "0     Commercial     IFRS Constant     21.929825         0.0  \n",
      "\n",
      "[1 rows x 52 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the column names\n",
    "columns = [\n",
    "    \"UnderwrittenCoverID\", \"PolicyID\", \"TransactionMonth\", \"IsVATRegistered\", \"Citizenship\", \"LegalType\", \n",
    "    \"Title\", \"Language\", \"Bank\", \"AccountType\", \"MaritalStatus\", \"Gender\", \"Country\", \"Province\", \"PostalCode\", \n",
    "    \"MainCrestaZone\", \"SubCrestaZone\", \"ItemType\", \"mmcode\", \"VehicleType\", \"RegistrationYear\", \"make\", \"Model\", \n",
    "    \"Cylinders\", \"cubiccapacity\", \"kilowatts\", \"bodytype\", \"NumberOfDoors\", \"VehicleIntroDate\", \n",
    "    \"CustomValueEstimate\", \"AlarmImmobiliser\", \"TrackingDevice\", \"CapitalOutstanding\", \"NewVehicle\", \"WrittenOff\", \n",
    "    \"Rebuilt\", \"Converted\", \"CrossBorder\", \"NumberOfVehiclesInFleet\", \"SumInsured\", \"TermFrequency\", \n",
    "    \"CalculatedPremiumPerTerm\", \"ExcessSelected\", \"CoverCategory\", \"CoverType\", \"CoverGroup\", \"Section\", \n",
    "    \"Product\", \"StatutoryClass\", \"StatutoryRiskType\", \"TotalPremium\", \"TotalClaims\"\n",
    "]\n",
    "\n",
    "# Read the file directly (adjust the path as necessary)\n",
    "df = pd.read_csv(r'..\\MachineLearningRating.txt', sep='|', header=None, names=columns, low_memory=False)\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(df.head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "484dd62f-6bdb-453b-b1c7-6f241b2fdfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bank                        145961\n",
      "AccountType                  40232\n",
      "MaritalStatus                 8259\n",
      "Gender                        9536\n",
      "mmcode                         552\n",
      "VehicleType                    552\n",
      "make                           552\n",
      "Model                          552\n",
      "Cylinders                      552\n",
      "cubiccapacity                  552\n",
      "kilowatts                      552\n",
      "bodytype                       552\n",
      "NumberOfDoors                  552\n",
      "VehicleIntroDate               552\n",
      "CustomValueEstimate         779642\n",
      "CapitalOutstanding               2\n",
      "NewVehicle                  153295\n",
      "WrittenOff                  641901\n",
      "Rebuilt                     641901\n",
      "Converted                   641901\n",
      "CrossBorder                 999400\n",
      "NumberOfVehiclesInFleet    1000098\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "missing_data = df.isnull().sum()\n",
    "print(missing_data[missing_data > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10a78f18-c80b-44c3-9b4e-7878746dad27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CraftSoft\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1231: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "# Impute numerical data with median\n",
    "numerical_columns = df.select_dtypes(include='number')\n",
    "for column in numerical_columns:\n",
    "    df[column] = df[column].fillna(df[column].median())\n",
    "\n",
    "# Impute categorical data with mode\n",
    "categorical_columns = df.select_dtypes(include='object')\n",
    "for category_column in categorical_columns:\n",
    "    df[category_column] = df[category_column].fillna(df[category_column].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2000e087-428f-4ddd-bd3d-e124dca7759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_year = datetime.now().year\n",
    "df['VehicleAge'] = current_year - df['RegistrationYear']\n",
    "df['VehicleAgeGroup'] = pd.cut(df['VehicleAge'], bins=[0, 1, 5, 10, 100], labels=['New', '1-5 years', '6-10 years', 'Over 10 years'])\n",
    "df['PremiumToClaimsRatio'] = df['TotalPremium'] / df['TotalClaims']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4a8d00f-59ba-4c2c-b1c1-1af483152b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.get_dummies(df, columns=['Citizenship', 'LegalType', 'Title', 'Language', 'Bank', 'AccountType', 'MaritalStatus', 'Gender', 'Country', 'Province', 'MainCrestaZone', 'SubCrestaZone', 'ItemType', 'make', 'Model', 'bodytype', 'CoverCategory', 'CoverType', 'CoverGroup', 'Section', 'Product', 'StatutoryClass', 'StatutoryRiskType'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1f0325c-11cc-47ff-87a5-0a8f7f826950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['IsVATRegistered'] = le.fit_transform(df['IsVATRegistered'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef40ce6f-08aa-4554-a23a-53a03d49d5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame Columns: Index(['UnderwrittenCoverID', 'PolicyID', 'TransactionMonth',\n",
      "       'IsVATRegistered', 'Citizenship', 'LegalType', 'Title', 'Language',\n",
      "       'Bank', 'AccountType', 'MaritalStatus', 'Gender', 'Country', 'Province',\n",
      "       'PostalCode', 'MainCrestaZone', 'SubCrestaZone', 'ItemType', 'mmcode',\n",
      "       'VehicleType', 'RegistrationYear', 'make', 'Model', 'Cylinders',\n",
      "       'cubiccapacity', 'kilowatts', 'bodytype', 'NumberOfDoors',\n",
      "       'VehicleIntroDate', 'CustomValueEstimate', 'AlarmImmobiliser',\n",
      "       'TrackingDevice', 'CapitalOutstanding', 'NewVehicle', 'WrittenOff',\n",
      "       'Rebuilt', 'Converted', 'CrossBorder', 'NumberOfVehiclesInFleet',\n",
      "       'SumInsured', 'TermFrequency', 'CalculatedPremiumPerTerm',\n",
      "       'ExcessSelected', 'CoverCategory', 'CoverType', 'CoverGroup', 'Section',\n",
      "       'Product', 'StatutoryClass', 'StatutoryRiskType', 'TotalPremium',\n",
      "       'TotalClaims'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CraftSoft\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1231: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Columns Present: ['Citizenship', 'LegalType', 'Title', 'Language', 'Bank', 'AccountType', 'MaritalStatus', 'Gender', 'Country', 'Province', 'MainCrestaZone', 'SubCrestaZone', 'ItemType', 'make', 'Model', 'bodytype', 'CoverCategory', 'CoverType', 'CoverGroup', 'Section', 'Product', 'StatutoryClass', 'StatutoryRiskType']\n",
      "Columns to encode: ['Citizenship', 'LegalType', 'Title', 'Language', 'Bank', 'AccountType', 'MaritalStatus', 'Gender', 'Country', 'Province', 'MainCrestaZone', 'SubCrestaZone', 'ItemType', 'make', 'Model', 'bodytype', 'CoverCategory', 'CoverType', 'CoverGroup', 'Section', 'Product', 'StatutoryClass', 'StatutoryRiskType']\n",
      "Columns after encoding: Index(['UnderwrittenCoverID', 'PolicyID', 'TransactionMonth',\n",
      "       'IsVATRegistered', 'PostalCode', 'mmcode', 'VehicleType',\n",
      "       'RegistrationYear', 'Cylinders', 'cubiccapacity',\n",
      "       ...\n",
      "       'Section_Motor Comprehensive', 'Section_Optional Extended Covers',\n",
      "       'Section_Standalone passenger liability',\n",
      "       'Section_Third party or third party, fire and theft only',\n",
      "       'Product_Bridge Taxi Finance: Monthly',\n",
      "       'Product_Mobility Commercial Cover: Monthly',\n",
      "       'Product_Mobility Metered Taxis: Monthly',\n",
      "       'Product_Standalone Passenger Liability', 'StatutoryClass_Commercial',\n",
      "       'StatutoryRiskType_IFRS Constant'],\n",
      "      dtype='object', length=682)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 13.7 MiB for an array with shape (9, 200020) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget variable \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotalPremium\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Train-Test Split\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Handle NaN values in training data\u001b[39;00m\n\u001b[0;32m     79\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mfillna(X_train\u001b[38;5;241m.\u001b[39mmean())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2805\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2801\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[0;32m   2803\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[1;32m-> 2805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2807\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\n\u001b[0;32m   2808\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2809\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2807\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2801\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[0;32m   2803\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[0;32m   2805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2806\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m-> 2807\u001b[0m         (_safe_indexing(a, train), \u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m   2808\u001b[0m     )\n\u001b[0;32m   2809\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:263\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecifying the columns using strings is only supported for dataframes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    258\u001b[0m     )\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# TODO: we should probably use _is_pandas_df_or_series(X) instead but this\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;66;03m# would require updating some tests such as test_train_test_split_mock_pandas.\u001b[39;00m\n\u001b[1;32m--> 263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pandas_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_polars_df_or_series(X):\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _polars_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:44\u001b[0m, in \u001b[0;36m_pandas_indexing\u001b[1;34m(X, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m     39\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(key)):\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# using take() instead of iloc[] ensures the return value is a \"proper\"\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# copy that will not raise SettingWithCopyWarning\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# check whether we should index with loc or iloc\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39miloc \u001b[38;5;28;01mif\u001b[39;00m key_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m X\u001b[38;5;241m.\u001b[39mloc\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:4133\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[1;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[0;32m   4128\u001b[0m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[0;32m   4129\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[0;32m   4130\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[0;32m   4131\u001b[0m     )\n\u001b[1;32m-> 4133\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4135\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4139\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4140\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:894\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[0;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:688\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    681\u001b[0m         indexer,\n\u001b[0;32m    682\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    683\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    684\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 688\u001b[0m         \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n\u001b[0;32m    698\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m    699\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:157\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    155\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n\u001b[0;32m    162\u001b[0m func(arr, indexer, out, fill_value)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 13.7 MiB for an array with shape (9, 200020) and data type float64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Load your DataFrame (replace this with your actual data loading method)\n",
    "# df = pd.read_csv('your_file.csv')  # Example of loading data\n",
    "\n",
    "# Check the columns in your DataFrame\n",
    "print(\"Original DataFrame Columns:\", df.columns)\n",
    "\n",
    "# Data Preparation\n",
    "# Fill missing values\n",
    "# Impute numerical data with median\n",
    "numerical_columns = df.select_dtypes(include='number')\n",
    "for column in numerical_columns:\n",
    "    df[column] = df[column].fillna(df[column].median())\n",
    "\n",
    "# Impute categorical data with mode\n",
    "categorical_columns = df.select_dtypes(include='object')\n",
    "for category_column in categorical_columns:\n",
    "    df[category_column] = df[category_column].fillna(df[category_column].mode()[0])\n",
    "\n",
    "# if 'RegistrationYear' in df.columns:\n",
    "#     df['RegistrationYear'] = df['RegistrationYear'].fillna(df['RegistrationYear'].median())\n",
    "# else:\n",
    "#     print(\"Column 'RegistrationYear' not found in DataFrame\")\n",
    "\n",
    "# if 'Bank' in df.columns:\n",
    "#     df['Bank'] = df['Bank'].fillna(df['Bank'].mode()[0])\n",
    "# else:\n",
    "#     print(\"Column 'Bank' not found in DataFrame\")\n",
    "\n",
    "# Feature Engineering and Encoding\n",
    "categorical_columns = [\n",
    "    'Citizenship', 'LegalType', 'Title', 'Language', 'Bank', 'AccountType',\n",
    "    'MaritalStatus', 'Gender', 'Country', 'Province', 'MainCrestaZone',\n",
    "    'SubCrestaZone', 'ItemType', 'make', 'Model', 'bodytype', 'CoverCategory',\n",
    "    'CoverType', 'CoverGroup', 'Section', 'Product', 'StatutoryClass', 'StatutoryRiskType'\n",
    "]\n",
    "\n",
    "# Check which categorical columns are present in the DataFrame\n",
    "existing_categorical_columns = [col for col in categorical_columns if col in df.columns]\n",
    "print(\"Categorical Columns Present:\", existing_categorical_columns)\n",
    "\n",
    "# Create a list of columns to exclude from encoding (including the target variable)\n",
    "exclude_columns = ['TotalPremium', 'TotalClaims']\n",
    "\n",
    "# Ensure target variable is not encoded\n",
    "columns_to_encode = [col for col in existing_categorical_columns if col not in exclude_columns]\n",
    "\n",
    "print(\"Columns to encode:\", columns_to_encode)\n",
    "\n",
    "if columns_to_encode:\n",
    "    # Encode categorical columns if they exist\n",
    "    df_encoded = pd.get_dummies(df, columns=columns_to_encode, drop_first=False)\n",
    "else:\n",
    "    print(\"No categorical columns found to encode.\")\n",
    "    df_encoded = df.copy()  # If no categorical columns, use the original DataFrame\n",
    "\n",
    "# Check the columns after encoding\n",
    "print(\"Columns after encoding:\", df_encoded.columns)\n",
    "\n",
    "# Ensure the target variable 'TotalPremium' is still present\n",
    "if 'TotalPremium' in df_encoded.columns:\n",
    "    X = df_encoded.drop(['TotalPremium', 'TotalClaims'], axis=1, errors='ignore')\n",
    "    y = df_encoded['TotalPremium']\n",
    "else:\n",
    "    raise ValueError(\"Target variable 'TotalPremium' not found in DataFrame\")\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle NaN values in training data\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_train.mean())  # Ensure the test set has the same NaN handling as the training set\n",
    "y_train = y_train.fillna(y_train.mean())\n",
    "y_test = y_test.fillna(y_train.mean())  # Ensure the test set has the same NaN handling as the training set\n",
    "\n",
    "# Modeling Techniques\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model_name, y_test, predictions):\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    print(f\"\\n{model_name} Evaluation Metrics:\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "    print(f\"R² Score: {r2}\")\n",
    "\n",
    "# 1. Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_predictions = lr_model.predict(X_test)\n",
    "evaluate_model(\"Linear Regression\", y_test, lr_predictions)\n",
    "\n",
    "# 2. Decision Tree Regressor\n",
    "dt_model = DecisionTreeRegressor()\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_predictions = dt_model.predict(X_test)\n",
    "evaluate_model(\"Decision Tree Regressor\", y_test, dt_predictions)\n",
    "\n",
    "# 3. Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "evaluate_model(\"Random Forest Regressor\", y_test, rf_predictions)\n",
    "\n",
    "# 4. XGBoost Regressor\n",
    "xgb_model = XGBRegressor()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_predictions = xgb_model.predict(X_test)\n",
    "evaluate_model(\"XGBoost Regressor\", y_test, xgb_predictions)\n",
    "\n",
    "# Feature Importance Analysis (for Random Forest and XGBoost)\n",
    "if hasattr(rf_model, 'feature_importances_'):\n",
    "    importances_rf = rf_model.feature_importances_\n",
    "    print(\"\\nFeature Importances (Random Forest):\")\n",
    "    for feature, importance in zip(X.columns, importances_rf):\n",
    "        print(f\"{feature}: {importance}\")\n",
    "\n",
    "if hasattr(xgb_model, 'feature_importances_'):\n",
    "    importances_xgb = xgb_model.feature_importances_\n",
    "    print(\"\\nFeature Importances (XGBoost):\")\n",
    "    for feature, importance in zip(X.columns, importances_xgb):\n",
    "        print(f\"{feature}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a1bd88-1a40-45c6-8c0b-256479616c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Check the columns in your DataFrame\n",
    "print(\"Original DataFrame Columns:\", df.columns)\n",
    "\n",
    "# Data Preparation\n",
    "# Fill missing values\n",
    "# Impute numerical data with median\n",
    "numerical_columns = df.select_dtypes(include='number')\n",
    "for column in numerical_columns:\n",
    "    df[column] = df[column].fillna(df[column].median())\n",
    "\n",
    "# Impute categorical data with mode\n",
    "categorical_columns = df.select_dtypes(include='object')\n",
    "for category_column in categorical_columns:\n",
    "    df[category_column] = df[category_column].fillna(df[category_column].mode()[0])\n",
    "# if 'RegistrationYear' in df.columns:\n",
    "#     df['RegistrationYear'] = df['RegistrationYear'].fillna(df['RegistrationYear'].median())\n",
    "# else:\n",
    "#     print(\"Column 'RegistrationYear' not found in DataFrame\")\n",
    "\n",
    "# if 'Bank' in df.columns:\n",
    "#     df['Bank'] = df['Bank'].fillna(df['Bank'].mode()[0])\n",
    "# else:\n",
    "#     print(\"Column 'Bank' not found in DataFrame\")\n",
    "\n",
    "# Define categorical columns\n",
    "categorical_columns = [\n",
    "    'Citizenship', 'LegalType', 'Title', 'Language', 'Bank', 'AccountType',\n",
    "    'MaritalStatus', 'Gender', 'Country', 'Province', 'MainCrestaZone',\n",
    "    'SubCrestaZone', 'ItemType', 'make', 'Model', 'bodytype', 'CoverCategory',\n",
    "    'CoverType', 'CoverGroup', 'Section', 'Product', 'StatutoryClass', 'StatutoryRiskType'\n",
    "]\n",
    "\n",
    "# Check which categorical columns are present in the DataFrame\n",
    "existing_categorical_columns = [col for col in categorical_columns if col in df.columns]\n",
    "print(\"Categorical Columns Present:\", existing_categorical_columns)\n",
    "\n",
    "# Use label encoding for columns with high cardinality and one-hot encoding for others\n",
    "high_cardinality_columns = [col for col in existing_categorical_columns if df[col].nunique() > 10]\n",
    "low_cardinality_columns = [col for col in existing_categorical_columns if col not in high_cardinality_columns]\n",
    "\n",
    "# Label Encoding for high cardinality columns\n",
    "label_encoder = LabelEncoder()\n",
    "for col in high_cardinality_columns:\n",
    "    df[col] = label_encoder.fit_transform(df[col].astype(str))\n",
    "\n",
    "# One-Hot Encoding for low cardinality columns\n",
    "if low_cardinality_columns:\n",
    "    df = pd.get_dummies(df, columns=low_cardinality_columns, drop_first=True)\n",
    "\n",
    "# Ensure the target variable 'TotalPremium' is still present\n",
    "if 'TotalPremium' in df.columns:\n",
    "    X = df.drop(['TotalPremium', 'TotalClaims'], axis=1, errors='ignore')\n",
    "    y = df['TotalPremium']\n",
    "else:\n",
    "    raise ValueError(\"Target variable 'TotalPremium' not found in DataFrame\")\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle NaN values in training data\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_train.mean())  # Ensure the test set has the same NaN handling as the training set\n",
    "y_train = y_train.fillna(y_train.mean())\n",
    "y_test = y_test.fillna(y_train.mean())  # Ensure the test set has the same NaN handling as the training set\n",
    "\n",
    "# Modeling Techniques\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree Regressor\": DecisionTreeRegressor(),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"XGBoost Regressor\": XGBRegressor()\n",
    "}\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model_name, y_test, predictions):\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    print(f\"\\n{model_name} Evaluation Metrics:\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "    print(f\"R² Score: {r2}\")\n",
    "\n",
    "# Training and evaluating each model\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    evaluate_model(model_name, y_test, predictions)\n",
    "\n",
    "# Feature Importance Analysis (for Random Forest and XGBoost)\n",
    "for model_name, model in models.items():\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        print(f\"\\nFeature Importances ({model_name}):\")\n",
    "        for feature, importance in zip(X.columns, importances):\n",
    "            print(f\"{feature}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294a1494-3965-4b7d-bfa0-612001e60468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
